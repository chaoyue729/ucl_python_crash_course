{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dimension Reduction & Clustering\n",
    "\n",
    "### ASI Data Science - 9th October 2017\n",
    "\n",
    "#### Alexander Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.cluster  import KMeans\n",
    "from sklearn.datasets import load_digits            \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.cluster.vq import kmeans\n",
    "\n",
    "from IPython.display  import Image as display_image\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image Segmentation\n",
    "\n",
    "In this first section, we introduce the k-means algorithm - the simplest clustering algorithm and its implementation in sklearn. As a simple application, we then show how k-means clustering can be used to segment an image by colour. By grouping similar (r,g,b) values together we should be able to extract different parts of the picture. We begin by loading an image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sp.misc.imread('./Landscape_small.png')\n",
    "display_image(filename='./Landscape_small.png', width = 380, height = 380)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above consists of (314 x 500) pixels. Each entry in the img array above encodes the colour values for each pixel. There are four values - the three primary (rgb) colours as well as an alpha (opacity) value. The opacity value is not relevant here so we'll discard it. We need to reshape the img array above in other to get it into a form that is suitable for the sklearn machine learning algorithms. These generally take two dimensional arrays where columns are regarded as features and rows as samples. We'll therefore reshape img into a three column array (r,g,b) with as many rows as there are pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = img[:, :, :3]\n",
    "\n",
    "img_flat = sp.reshape(img, (img.shape[0]*img.shape[1],3)).astype(float)\n",
    "\n",
    "print(img.shape, img_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering in sklearn. Notice the similarity in syntax to the classifiers used in supervised learning. kmeans requires the number of clusters as input. The fit method calculates the centroids based on the input data, whilst the predict method assigns cluster membership to each point. Notice that once fitted, the predict method is able to take new data - so you can assign cluster membership to new points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K-means in sklearn.\n",
    "\n",
    "clf = KMeans(n_clusters = 3, init='k-means++')\n",
    "\n",
    "clf.fit(img_flat.astype(float)) \n",
    "\n",
    "indices= clf.predict(img_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the image pack into 3D and pass back to plt.imshow()\n",
    "\n",
    "plt.imshow(sp.reshape(indices, (img.shape[0], img.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means algorithm requires the user to input the number of clusters. One issue is that in general we do not know a-priori what this value should be (unless we have application specific information). We should choose the number of clusters such that adding an additional cluster no longer gives 'better' modelling of the data. We can do this by plotting an objective function against the number of clusters and chosing the optimal value to be the point where the curve has a visible kink/elbow - indicating that adding additional clusters gives diminishing returns. Another different approach is to optimise a quantity known as the average silhouette coefficient. This is something you can read about here:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "The code below shows how to generate such an 'elbow' plot using the average within cluster sum of squares as the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids=[[] for i in range(1,10)]\n",
    "euclidean_distances=[0 for i in range(len(img_flat))]\n",
    "cluster_averaged_distances=[0 for i in range(1,10)]\n",
    "\n",
    "for K in range(1,10):\n",
    "    clf = KMeans(n_clusters = K, init='k-means++')\n",
    "    clf.fit(img_flat.astype(float))\n",
    "    \n",
    "    indices= clf.predict(img_flat)    \n",
    "    centroids= clf.cluster_centers_ \n",
    "    \n",
    "    for j in range(len(img_flat)):\n",
    "        euclidean_distances[j]=0\n",
    "        cluster=indices[j]\n",
    "        for k in range(3):\n",
    "            euclidean_distances[j]+=(img_flat[j][k] - centroids[cluster][k])**2\n",
    "        euclidean_distances[j]=(euclidean_distances[j])**0.5\n",
    "\n",
    "    cluster_averaged_distances[K-1]=np.mean(euclidean_distances)\n",
    "               \n",
    "plt.plot(range(1,10),cluster_averaged_distances)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "tt = plt.title('Example Elbow Curve')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, the loops in the calculation above make it very inefficient. There is a faster 'black box' implementation available in scipy (that uses a very slightly different objective function). The scipy method holds the results in a somewhat different format. It stores the coordinates of all centroids together with the average distance of points from their closest centroid (which is what we're after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1,10)\n",
    "\n",
    "KM = [kmeans(img_flat,k) for k in K] # Note the different call below to kmeans. This is scipy and not sklearn\n",
    "\n",
    "distances=[KM[i][1] for i in range(len(KM))]\n",
    "\n",
    "plt.plot(range(1,10),distances)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "tt = plt.title('Elbow for K-Means clustering') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: What's the optimal number of clusters above? Repeat the steps above with your favourite .png image, see if you can segment the colours that make up the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Digits and Dimension Reduction\n",
    "\n",
    "We now look at the sklearn digits dataset. These are images of hand-written digits. A classic problem (and now benchmmark for models) in machine learning is whether it is possible to train a classifier to distinguish between these different digits. This dataset is labelled but here we'll look at the extent to which purely unsupervised methods are able to segment this dataset. Each digit is an (8x8) collection of pixels and there are a total of 1797 samples in the dataset. This particular digits dataset can be regarded as a low dimensional analogue of the famous MNIST dataset. You can apply the exact same methods used here to that much higher dimensional case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "\n",
    "X = np.vstack([digits.data[digits.target==i]\n",
    "               for i in range(10)])\n",
    "y = np.hstack([digits.target[digits.target==i]\n",
    "               for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about the details of this plotting function. It's simply the show the handwritten digits to you.\n",
    "nrows, ncols = 2, 5\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.gray()\n",
    "for i in range(ncols * nrows):\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    ax.matshow(digits.images[i,...])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title(digits.target[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For what follows it's useful to build a 2D scatter plot function that takes (data, labels) as arguments\n",
    "import seaborn as sns \n",
    "\n",
    "def scatter(x, colors):\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "\n",
    "    return f, ax, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) in sklearn. The algorithm requires you to specify the number of principal components that you wish to retain. Note that PCA has both fit() and transform() methods. The former calculates the SVD of the input data and the necessary coordinate transformation to reduce the desired number of dimensions, whilst the latter actually applies the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=15)                           \n",
    "X_pca = pca.fit_transform(X)\n",
    "scatter(X_pca, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with clustering, it is important to have a mechanism to determine the correct number of principal components. In the case of PCA, the important quantity is the fraction of the total variance maintained as a function of the number of principal components retained. It is not uncommon to find in high dimensional dimension datasets that nearly all of the total variance can be retained whilst maintaining only a few principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.ylabel('Fraction of Variance Retained')\n",
    "plt.xlabel('Number of PCA Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: To what extent is PCA able to separate the digits dataset? To answer this question, first decide how many principal components you want to retain, then run k-means clustering on the result. Since the digits dataset is labelled, you can compare how closely your clustering labels match the true labels. You will find the following useful when comparing the labels:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other dimensional reduction algorithms that can in certain situations perform better than PCA. The study of these falls under the topic of manifold learning. Such algorithms attempt to preserve non-linear information and structure from the original higher dimensional space. Many are for example sensitive to the topology of that space - such as whether it has holes. PCA in contrast is a purely linear operation - it knows nothing about this structure. One particularly powerful example of manifold learning is an algorithm called t-SNE (t-distributed stochastic neighbour embedding). The details underpinning it are fairly involved and we won't talk about them here. As you'll see if you do the exercise below, it is very dramatically able to separate the digits dataset. You can read more about the algorithm here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II: To what extent is t-SNE able to separate the digits dataset? (The syntax for t-SNE is shown commented out below to help you). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# X_tsne = TSNE(random_state = 42).fit_transform(X)\n",
    "# scatter(X_tsne, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III: Use the cluster labels obtained from t-SNE + k-means as labels for supervised learning. Split the digits dataset (X,y) into a training and test set. Train a classifier (e.g. a random forest) to predict these cluster labels and validate it on the test set. For this digits dataset we actually have access to the 'true' labels so we can check how accurate the final results are. Remember that cluster label 0 is not necessarily equal to digit 0 so you'll want to work out what digit each cluster corresponds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other Clustering Algorithms\n",
    "\n",
    "Clustering is an enormous subject and we have barely scratched the surface this evening. k-means (and its variations) are often a very good place to start, but in many practical situations it will be insufficient. This final section shows you some examples of where k-means fails to cluster correctly and invites you to investigate the behaviour of various other clustering algorithms in sklearn. We compare their ability to cluster four different datasets. The plotting function below will also give you some indication of the relative speeds of the different clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate four standard datasets for clustering. \n",
    "\n",
    "n_samples = 1500\n",
    "\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A plotting function that will be used below. Do not worry about the details of this.\n",
    "\n",
    "def PlotClustering(clustering_names, clustering_algorithms):\n",
    "    \n",
    "    plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))\n",
    "\n",
    "    plot_num = 1\n",
    "\n",
    "    datasets = [noisy_circles, noisy_moons, blobs, no_structure]\n",
    "    for i_dataset, dataset in enumerate(datasets):\n",
    "        X, y = dataset\n",
    "        X = StandardScaler().fit_transform(X) # NOTE: Normalise your data before clustering\n",
    "\n",
    "        for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "            # Predict cluster memberships\n",
    "            plt.subplot(4, len(clustering_algorithms), plot_num)\n",
    "            if i_dataset == 0:\n",
    "                plt.title(name, size=18)\n",
    "\n",
    "            if algorithm is None:\n",
    "                plt.scatter(X[:, 0], X[:, 1], color='gray', s=10)\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                algorithm.fit(X)\n",
    "                t1 = time.time()\n",
    "                if hasattr(algorithm, 'labels_'):\n",
    "                    y_pred = algorithm.labels_.astype(np.int)\n",
    "                else:\n",
    "                    y_pred = algorithm.predict(X)\n",
    "\n",
    "                # Plot\n",
    "                plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
    "\n",
    "                if hasattr(algorithm, 'cluster_centers_'):\n",
    "                    centers = algorithm.cluster_centers_\n",
    "                    center_colors = colors[:len(centers)]\n",
    "                    plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "            if algorithm is not None:\n",
    "                plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                         transform=plt.gca().transAxes, size=15,\n",
    "                         horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Test different clustering algorithms on the four datasets below. You'll find they have different behaviours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE GOES HERE ########\n",
    "###################################\n",
    "\n",
    "# FIND SOME CLUSTERING ALGORITHMS THAT BEST SEPARATE THIS DATA.\n",
    "# You can see from what I've shown below that kmeans doesn't work very well here. \n",
    "\n",
    "###################################\n",
    "###################################\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters = 2)\n",
    "\n",
    "names = ['Raw Data', 'kmeans']\n",
    "algorithms = [None, kmeans] \n",
    "\n",
    "PlotClustering(names, algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
